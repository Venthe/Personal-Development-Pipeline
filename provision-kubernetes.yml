# ansible-galaxy collection install community.general ansible.posix kubernetes.core
---
- name: Prepare for elasticsearch containers
  # gather_facts: true
  hosts: all
  tasks:
    - name: Raise "vm.max_map_count"
      become: true
      ansible.posix.sysctl:
        name: "vm.max_map_count"
        value: "262144"
        state: present
        sysctl_set: true
        reload: true
- name: "Install and configure prerequisites for containerd CRI: Forwarding IPv4 and letting iptables see bridged traffic"
  # gather_facts: false
  # become: true
  hosts: all
  # vars_prompt:
  #   - name: chown_user
  #     default: vagrant
  #     private: false
  tasks:
      # cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
      # overlay
      # br_netfilter
      # EOF

      # sudo modprobe overlay
      # sudo modprobe br_netfilter
    - name: "Activate the required kernel modules"
      become: true
      community.general.modprobe:
        name: "{{ item }}"
      with_items:
        - overlay
        - br_netfilter
    - name: Load kernel modules after restart
      become: true
      copy:
        mode: '0644'
        dest: "/etc/modules-load.d/containerd.conf"
        content: |
          overlay
          br_netfilter
    # # sysctl params required by setup, params persist across reboots
    # cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
    # net.bridge.bridge-nf-call-iptables  = 1
    # net.bridge.bridge-nf-call-ip6tables = 1
    # net.ipv4.ip_forward                 = 1
    # EOF

    # # Apply sysctl params without reboot
    # sudo sysctl --system
    - name: Set sysctl for network forwarding
      become: true
      ansible.posix.sysctl:
        name: "{{ item }}"
        value: '1'
        state: present
        sysctl_file: /etc/sysctl.d/k8s.conf
      with_items:
        - "net.bridge.bridge-nf-call-iptables"
        - "net.bridge.bridge-nf-call-ip6tables"
        - "net.ipv4.ip_forward"
- name: Install containerd CRI
  hosts: all
  gather_facts: true
  vars:
    containerd_version: "1.6.6-1"
  tasks:
    - name: Update APT cache
      become: true
      apt:
        update_cache: true
    - name: Install packages that allow apt to be used over HTTPS
      become: true
      apt:
        name: "{{ packages }}"
        state: present
        update_cache: false
      vars:
        packages:
          - apt-transport-https
          - ca-certificates
          - curl
          - software-properties-common
    - name: Add an apt signing key for Docker repository
      become: true
      apt_key:
        url: https://download.docker.com/linux/{{ ansible_distribution | lower }}/gpg
        state: present
    - name: Add apt repository for Docker
      become: true
      apt_repository:
        repo: "deb [arch=amd64] https://download.docker.com/linux/{{ ansible_distribution | lower }} {{ ansible_distribution_release }} stable"
        state: present
        filename: docker
    - name: Install containerd
      become: true
      apt:
        name: "containerd.io={{ containerd_version }}"
        state: present
        update_cache: true
    - name: Create directory for containerd configuration
      become: true
      file:
        mode: '0755'
        path: /etc/containerd/
        state: directory
    - name: Prepare default containerd configuration
      command: containerd config default
      register: default_containerd_config
    - name: Save containerd configuration
      become: true
      copy:
        mode: '0755'
        content: "{{ default_containerd_config.stdout_lines | join('\n') }}"
        dest: /etc/containerd/config.toml
    - name: Set containerd configuration
      become: true
      ansible.builtin.lineinfile:
        path: /etc/containerd/config.toml
        regexp: "{{ item.regexp }}"
        line: "{{ item.line }}"
        backrefs: yes
      with_items:
        - regexp: '^(disabled_plugins\s*=\s*\[s*)(\"cri\")(\s*\])$'
          line: '\1\3'
        - regexp: '^(\s*config_path)\s*=\s*.*$'
          line: '\1 = "/etc/containerd/certs.d"'
        - regexp: '^(\s*SystemdCgroup)\s*=\s*.*$'
          line: \1 = true
    - name: Add an apt signing key for opensuse repository
      become: true
      apt_key:
        url: https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/Debian_11/Release.key
        state: present
    - name: Add devel:kubic:libcontainers:stable
      become: true
      ansible.builtin.apt_repository:
        repo: "deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/Debian_11/ /"
        state: present
        filename: "devel:kubic:libcontainers:stable"
    - name: Install crictl
      become: true
      apt:
        name: cri-tools
        state: present
        update_cache: true
    - name: Configure crictl to use containerd
      become: true
      copy:
        mode: '0644'
        dest: "/etc/crictl.yaml"
        content: |
          runtime-endpoint: unix:///var/run/containerd/containerd.sock
          image-endpoint: unix:///var/run/containerd/containerd.sock
          timeout: 10
          debug: false
    - name: Restart containerd
      become: true
      service:
        name: containerd
        state: restarted
- name: Set containerd snapshotter to work with ZFS
  hosts: zfs
  tasks:
  - name: Creates directory
    file:
      path: /var/lib/containerd/io.containerd.snapshotter.v1.zfs/
      state: directory
  - name: Create a zvol for snapshots
    become: "true"
    community.general.zfs:
      name: "main/containerd-snapshots"
      state: present
      extra_zfs_properties:
        mountpoint: /var/lib/containerd/io.containerd.snapshotter.v1.zfs
  - name: Set containerd configuration
    become: true
    ansible.builtin.lineinfile:
      path: /etc/containerd/config.toml
      regexp: '^(\s*snapshotter)\s*=\s*.*$'
      line: '\1 = "zfs"'
      backrefs: yes
    - name: Restart containerd
      become: true
      ansible.builtin.service:
        enabled: true
        name: containerd
        daemon_reload: true
        state: restarted
- name: Install kubernetes
  gather_facts: false
  become: true
  hosts: all
  vars:
    kubernetes_version: "1.24.3-00"
  tasks:
    # curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
    - name: Add an apt signing key for Kubernetes
      apt_key:
        url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
        state: present
    # sudo add-apt-repository 'deb https://apt.kubernetes.io/ kubernetes-xenial main'
    - name: Adding apt repository for Kubernetes
      apt_repository:
        repo: deb https://apt.kubernetes.io/ kubernetes-xenial main
        state: present
        filename: kubernetes
    # sudo apt install kubelet kubeadm kubectl=2-00 kubectl=1.24.0-00
    # Truenas: sudo apt --fix-broken -o Dpkg::Options::="--force-overwrite" install
    - name: Install Kubernetes binaries
      apt:
        name: "{{ packages }}"
        state: present
        update_cache: true
      vars:
        packages:
          - "kubeadm={{ kubernetes_version }}"
          - "kubelet={{ kubernetes_version }}"
          - "kubectl={{ kubernetes_version }}"
    # sudo apt-mark hold kubeadm kubelet kubectl
    - name: APT Hold kebernetes packages
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      with_items:
        - kubeadm
        - kubelet
        - kubectl
    # TODO: Set up audit logs
    # # Preparation for audit logs
    # - name: Create a directory if it does not exist
    #   file:
    #     mode: '0644'
    #     owner: 'root'
    #     path: "{{ item }}"
    #     state: directory
    #   with_items:
    #     - /var/log/kubernetes/audit
    #     - /etc/kubernetes
    # - name: Set up audit policy
    #   copy:
    #     mode: '0644'
    #     owner: 'root'
    #     dest: "/etc/kubernetes/audit-policy.yaml"
    #     content: |
    #       apiVersion: audit.k8s.io/v1beta1
    #       kind: Policy
    #       rules:
    #       - level: Metadata
# RESET
# sudo kubeadm init phase certs all
# sudo ip route flush proto bird
# sudo rm -rf /var/etcd/calico-data/*
# sudo cp /etc/kubernetes/pki/ca.crt /usr/local/share/ca-certificates/kubernetes.crt && sudo update-ca-certificates
- name: Set up control plane
  gather_facts: true
  hosts: main
  vars:
    taint_control_plane: false
  tasks:
    - set_fact:
        ip: "{{ item }}"
      with_items: "{{ ansible_all_ipv4_addresses }}"
      when: "item.startswith('192') or item.startswith('172')"
    - name: Prepare init configuration
      become: true
      blockinfile:
        create: yes
        backup: yes
        dest: /var/lib/kubelet/config.init.yaml
        block: |
          apiVersion: kubeadm.k8s.io/v1beta2
          kind: ClusterConfiguration
          # --control-plane-endpoint
          controlPlaneEndpoint: {{ ip }}:6443
          ---
          apiVersion: kubelet.config.k8s.io/v1beta1
          kind: KubeletConfiguration
          cgroupDriver: systemd
          memorySwap:
            swapBehavior: LimitedSwap
          featureGates:
            NodeSwap: true
          failSwapOn: false
          networking:
            # --pod-network-cidr
            podSubnet: "10.244.0.0/16" 
          localAPIEndpoint:
            # --apiserver-advertise-address
            advertiseAddress: {{ ip }}
            bindPort: 6443
          ...
    - name: Restart kubelet
      become: true
      service:
        name: kubelet
        daemon_reload: true
        state: restarted
    # # Fixme: Run this only when necessary
    # #  kubeadm config images list -o jsonpath='{range .images[*]}{@}{"\n"}{end}'
    # #  Store available images as a fact?
    #- name: Pull config images
    #  become: true
    #  command: kubeadm config images pull
    # Fixme: Run this only when necessary
    - name: Initialize the Kubernetes cluster using kubeadm
      become: true
      # --apiserver-advertise-address="192.168.50.10"
      #  The IP address the API Server will advertise it's listening on.
      #  If not set the default network interface will be used
      # --contorl-plane-endpoint
      #  Specify a stable IP address or DNS name for the control plane.
      # --apiserver-cert-extra-sans="192.168.50.10"
      #  Optional extra Subject Alternative Names (SANs) to use for the API
      #  Server serving certificate. Can be both IP addresses and DNS names.
      # --node-name {{ node }}
      #  Specify the node name.
      # --pod-network-cidr=192.168.0.0/16
      #  Specify range of IP addresses for the pod network. If set, the
      #  control plane will automatically allocate CIDRs for every node.
      # --service-dns-domain="{{ cluster_domain }}" \
      shell:
        #--apiserver-cert-extra-sans="{{ ip }},127.0.0.1,localhost,{{ansible_hostname}}" \
        # --service-cidr=10.196.0.0/16 \
        # calico pod network cidr: 192.168.0.0/16.
        # --apiserver-cert-extra-sans="127.0.0.1" \
        # --apiserver-cert-extra-sans="127.0.0.1" \
        # --apiserver-cert-extra-sans="127.0.0.1" \
        cmd: |
          kubeadm init \
              --config=/var/lib/kubelet/config.init.yaml \
              --node-name={{ ansible_hostname }} \
              --skip-token-print
    - name: Create .kube directory for user
      become: true
      file:
        path: "{{ ansible_env.HOME }}/.kube"
        state: directory
        mode: '0700'
        owner: "{{ ansible_env.LOGNAME }}"
    - name: Setup kubeconfig for user
      become: true
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "{{ ansible_env.HOME }}/.kube/config"
        owner: "{{ ansible_env.LOGNAME }}"
        remote_src: true
        mode: '0700'
    - name: Generate join command
      command: kubeadm token create --print-join-command
      register: join_command
      changed_when: false
      # Change to fetch
    - name: Copy join command to local file
      delegate_to: localhost
      become: false
      copy:
        content: "{{ join_command.stdout_lines[0] }}"
        mode: '0777'
        dest: "./join-command"
    - name: Untaint node
      command: "kubectl taint node {{ ansible_hostname }} {{ item }}-"
      when: not taint_control_plane
      with_items:
        - node-role.kubernetes.io/control-plane
        - node-role.kubernetes.io/master
- name: Set up worker node
  hosts: worker
  tasks:
  - name: Copy the join command to server location
    become: true
    copy:
      src: join-command
      dest: /tmp/join-command.sh
      mode: 0777
  # Run only as needed
  - name: Join the node to cluster
    become: true
    command: sh /tmp/join-command.sh
- name: "Install CNI: flannel"
  gather_facts: false
  become: false
  hosts: main
  tasks:
    - name: Create temporary directory
      ansible.builtin.tempfile:
        state: directory
        suffix: temporary
      register: temporary
    # sudo mkdir -p /opt/bin && sudo wget https://github.com/flannel-io/flannel/releases/download/v0.19.0/flanneld-amd64 --quiet --output-file=/opt/bin/flanneld && sudo chmod +x /opt/bin/flanneld
    - name: Download manifest to the cluster.
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
        dest: "{{ temporary.path }}/flannel.yaml"
        mode: '0664'
    # Fixme: Run this only when necessary
    - name: Install pod network
      kubernetes.core.k8s:
        state: present
        src: "{{ temporary.path }}/flannel.yaml"
    - name: Use the registered var and the file module to remove the temporary file
      ansible.builtin.file:
        path: "{{ temporary.path }}"
        state: absent
      when: temporary.path is defined
- name: "Install CNI: calico"
  gather_facts: false
  become: false
  hosts: main
  tasks:
    - name: Create temporary directory
      ansible.builtin.tempfile:
        state: directory
        suffix: temporary
      register: temporary
    - name: Download calico manifest to the cluster.
      ansible.builtin.get_url:
        url: https://docs.projectcalico.org/manifests/calico.yaml
        dest: "{{ temporary.path }}/calico.yaml"
        mode: '0664'
    # Fixme: Run this only when necessary
    - name: Install calico pod network
      kubernetes.core.k8s:
        state: present
        src: "{{ temporary.path }}/calico.yaml"
    - name: Use the registered var and the file module to remove the temporary file
      ansible.builtin.file:
        path: "{{ temporary.path }}"
        state: absent
      when: temporary.path is defined
- name: Install Helm
  gather_facts: false
  hosts: all
  tasks:
    - name: Add helm repository signing key
      become: true
      apt_key:
        url: https://baltocdn.com/helm/signing.asc
        # state: present
    - name: Add helm apt repository
      become: true
      apt_repository:
        repo: deb https://baltocdn.com/helm/stable/debian/ all main
        filename: helm
    - name: Install Helm
      become: true
      apt:
        name: "helm"
    - name: Install Helm diff plugin
      kubernetes.core.helm_plugin:
        plugin_path: https://github.com/databus23/helm-diff
        state: present
- name: Install jq
  gather_facts: false
  hosts: all
  tasks:
    - name: Install jq
      become: true
      apt:
        name: "jq"
- name: "Deploy CSI: NFS"
  gather_facts: false
  hosts: nfs
  vars_prompt:
    - name: nfsServer.apiKey
      private: true
    - name: nfsServer.ssh.key
      private: true
    - name: nfsServer.host
      default: truenas
  vars:
    nfsServer:
      ssh:
        username: root
      namespace: csi
      username: root
  tasks:
    # TODO: Do ZFS configuration
    # - name: Create a zvol for snapshots
    #   become: "true"
    #   community.general.zfs:
    #     #-snapshots
    #     name: "{{item}}"
    #     state: present
    #   with_items:
    #     - main/k8s
    #     - main/k8s/snapshots
    #     - main/k8s/volumes
    - name: Install CIFS utils
      become: true
      apt:
        name: "nfs-common"
        state: present
        update_cache: true
    - kubernetes.core.helm_repository:
        name: democratic-csi
        repo_url: https://democratic-csi.github.io/charts/
    - kubernetes.core.helm:
        create_namespace: true
        update_repo_cache: true
        name:  truenas-zfs-csi
        chart_ref: democratic-csi/democratic-csi
        release_namespace: "{{ nfsServer.namespace }}"
        values: "{{ lookup('template', './values/democratic-csi-freenas-api-nfs.yaml') | from_yaml }}"
- name: Set up local ZFS CSI
  hosts: zfs
  vars:
    storageClass: zfs-local-dataset
  tasks:
    - kubernetes.core.helm_repository:
        name: democratic-csi
        repo_url: https://democratic-csi.github.io/charts/
    - name: Create a zvol for dataset
      become: true
      community.general.zfs:
        name: "{{ item }}"
        state: present
      with_items:
        - main/k8s/zfs-local/volumes
        - main/k8s/zfs-local/snapshots
    - name: Deploy latest version of zfs-local-dataset chart inside democratic-csi namespace with values
      kubernetes.core.helm:
        create_namespace: true
        name: "{{ storageClass }}"
        chart_ref: democratic-csi/democratic-csi
        release_namespace: democratic-csi
        values: "{{ lookup('template', './values/csi-democratic-csi-zfs-local.yaml') | from_yaml }}"
- name: Install metallb
  gather_facts: false
  hosts: main
  vars:
    bits_in_secret: 512
    metallb_version: v0.12.1
    address_range:
      from: 172.22.10.1
      to: 172.22.15.254
  tasks:
    - command: kubectl get configmap kube-proxy --namespace kube-system -o yaml
      register: configmap
    # If youâ€™re using kube-proxy in IPVS mode, since Kubernetes v1.14.2 you have to enable strict ARP mode.
    - name: Enable strict ARP
      k8s:
        state: present
        definition: "{{configmap.stdout | replace ('strictARP: false','strictARP: true') | replace ('mode: \"\"','mode: \"ipvs\"') }}"
    - k8s:
        state: present
        definition: |
          apiVersion: v1
          kind: Namespace
          metadata:
            name: loadbalancer
            labels:
              pod-security.kubernetes.io/enforce: privileged
              pod-security.kubernetes.io/audit: privileged
              pod-security.kubernetes.io/warn: privileged
    - kubernetes.core.helm_repository:
        name: bitnami
        repo_url: https://charts.bitnami.com/bitnami
    - kubernetes.core.helm:
        name: metallb
        chart_ref: bitnami/metallb
        release_namespace: loadbalancer
        values:
          configInline:
            address-pools:
            - name: default
              protocol: layer2
              addresses:
              - "{{ address_range.from }}-{{ address_range.to }}"
- name: Install ingress
  gather_facts: false
  hosts: main
  tasks:
    - name: "Delete namespace"
      k8s:
        state: absent
        kind: Namespace
        name: ingress
        api_version: v1
    - kubernetes.core.helm_repository:
        name: ngress-nginx
        repo_url: https://kubernetes.github.io/ingress-nginx
    - kubernetes.core.helm:
        name: ingress
        create_namespace: true
        chart_ref: ingress-nginx/ingress-nginx
        release_namespace: ingress
        values:
          controller:
            service:
              annotations:
                external-dns.alpha.kubernetes.io/hostname: "home.arpa"
    - name: Instructions
      debug:
        msg: "To use, add the 'kubernetes.io/ingress.class: nginx' annotation to ingress"
- name: Install dashboard
  gather_facts: false
  hosts: main
  vars:
    deployment_namespace: dashboard
    deployment_name: dashboard
    tld_hostname: home.arpa
  tasks:
    - name: "Delete namespace"
      k8s:
        state: absent
        kind: Namespace
        name: "{{ deployment_namespace }}"
        api_version: v1
    - kubernetes.core.helm_repository:
        name: kubernetes-dashboard
        repo_url: https://kubernetes.github.io/dashboard/
    - kubernetes.core.helm:
        name: "{{ deployment_name }}"
        create_namespace: true
        chart_ref: kubernetes-dashboard/kubernetes-dashboard
        release_namespace: "{{ deployment_namespace }}"
        values: "{{ lookup('template', './values/dashboard-values.yaml') | from_yaml }}"
    - k8s:
        definition: "{{ lookup('template', './values/dashboard-user-{{ item }}.yaml') | from_yaml }}"
        state: present
      with_items:
        - crb
        - sa
    - name: Password to dashboard
      shell:
        cmd: "kubectl -n {{ deployment_namespace }} create token admin-user"
      register: dashboard_password
    - debug:
        msg: "{{dashboard_password.stdout}}"
# TODO: Make ldap declarative
- name: Deploy LDAP
  gather_facts: true
  hosts: main
  vars:
    LDAP_BASE_DN: "{{'{{ LDAP_BASE_DN }}'}}"
    LDAP_DOMAIN: "{{'{{ LDAP_DOMAIN }}'}}"
    LDAP_READONLY_USER_USERNAME: "{{'{{ LDAP_READONLY_USER_USERNAME }}'}}"
    LDAP_READONLY_USER_PASSWORD_ENCRYPTED: "{{'{{ LDAP_READONLY_USER_PASSWORD_ENCRYPTED }}'}}"
    tld_hostname: home.arpa
    ldap:
      ldif: "{{ lookup('template', './values/ldap.ldif') }}"
      namespace: ldap
      organization: "My Company"
      openldap:
        releaseName: openldap
      phpldapadmin:
        releaseName: phpldapadmin
  vars_prompt:
    - name: admin_password
      default: "secret"
      private: yes
      prompt: Ldap admin password
  tasks:
    - kubernetes.core.helm_repository:
        name: "{{ item.name }}"
        repo_url: "{{ item.url }}"
      with_items:
        - name: stable
          url: https://charts.helm.sh/stable
        - name: cetic
          url: https://cetic.github.io/helm-charts
    - kubernetes.core.helm:
        name: dummy
        namespace: kube-system
        state: absent
        update_repo_cache: true
    - kubernetes.core.helm:
        name: "{{ item.name }}"
        create_namespace: true
        chart_ref: "{{ item.chartRef }}"
        release_namespace: "{{ ldap.namespace }}"
        values: "{{ lookup('template', './values/ldap-{{ item.name }}.yml') | from_yaml }}"
      with_items:
        - name: "{{ ldap.phpldapadmin.releaseName }}"
          chartRef: cetic/phpldapadmin
        - name: "{{ ldap.openldap.releaseName }}"
          chartRef: stable/openldap
    - kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: phpldapadmin-ingress
            namespace: "{{ ldap.namespace }}"
            annotations:
              kubernetes.io/ingress.class: "nginx"
              cert-manager.io/cluster-issuer: ca-issuer
          spec:
            rules:
              - host: "ldap-admin.{{ tld_hostname }}"
                http:
                  paths:
                    - backend:
                        service:
                          name: "{{ ldap.phpldapadmin.releaseName }}"
                          port:
                            number: 80
                      pathType: ImplementationSpecific
            tls:
              - hosts:
                  - "ldap-admin.{{ tld_hostname }}"
                secretName: ldap-admin-tls
...