# ansible-galaxy collection install community.general ansible.posix kubernetes.core
---
- name: Prepare for elasticsearch containers
  # gather_facts: true
  hosts: all
  tasks:
    - name: Raise "vm.max_map_count"
      become: true
      ansible.posix.sysctl:
        name: "vm.max_map_count"
        value: "262144"
        state: present
        sysctl_set: true
        reload: true
- name: "Install and configure prerequisites for containerd CRI: Forwarding IPv4 and letting iptables see bridged traffic"
  # gather_facts: false
  # become: true
  hosts: all
  # vars_prompt:
  #   - name: chown_user
  #     default: vagrant
  #     private: false
  tasks:
      # cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
      # overlay
      # br_netfilter
      # EOF

      # sudo modprobe overlay
      # sudo modprobe br_netfilter
    - name: "Activate the required kernel modules"
      become: true
      community.general.modprobe:
        name: "{{ item }}"
      with_items:
        - overlay
        - br_netfilter
    - name: Load kernel modules after restart
      become: true
      copy:
        mode: '0644'
        dest: "/etc/modules-load.d/containerd.conf"
        content: |
          overlay
          br_netfilter
    # # sysctl params required by setup, params persist across reboots
    # cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
    # net.bridge.bridge-nf-call-iptables  = 1
    # net.bridge.bridge-nf-call-ip6tables = 1
    # net.ipv4.ip_forward                 = 1
    # EOF

    # # Apply sysctl params without reboot
    # sudo sysctl --system
    - name: Set sysctl for network forwarding
      become: true
      ansible.posix.sysctl:
        name: "{{ item }}"
        value: '1'
        state: present
        sysctl_file: /etc/sysctl.d/k8s.conf
      with_items:
        - "net.bridge.bridge-nf-call-iptables"
        - "net.bridge.bridge-nf-call-ip6tables"
        - "net.ipv4.ip_forward"
- name: Install containerd CRI
  hosts: all
  gather_facts: true
  vars:
    containerd_version: "1.6.6-1"
  tasks:
    - name: Update APT cache
      become: true
      apt:
        update_cache: true
    - name: Install packages that allow apt to be used over HTTPS
      become: true
      apt:
        name: "{{ packages }}"
        state: present
        update_cache: false
      vars:
        packages:
          - apt-transport-https
          - ca-certificates
          - curl
          - software-properties-common
    - name: Add an apt signing key for Docker repository
      become: true
      apt_key:
        url: https://download.docker.com/linux/{{ ansible_distribution | lower }}/gpg
        state: present
    - name: Add apt repository for Docker
      become: true
      apt_repository:
        repo: "deb [arch=amd64] https://download.docker.com/linux/{{ ansible_distribution | lower }} {{ ansible_distribution_release }} stable"
        state: present
        filename: docker
    - name: Install containerd
      become: true
      apt:
        name: "containerd.io={{ containerd_version }}"
        state: present
        update_cache: true
    - name: Create directory for containerd configuration
      become: true
      file:
        mode: '0755'
        path: /etc/containerd/
        state: directory
    - name: Prepare default containerd configuration
      command: containerd config default
      register: default_containerd_config
    - name: Save containerd configuration
      become: true
      copy:
        mode: '0755'
        content: "{{ default_containerd_config.stdout_lines | join('\n') }}"
        dest: /etc/containerd/config.toml
    - name: Set containerd configuration
      become: true
      ansible.builtin.lineinfile:
        path: /etc/containerd/config.toml
        regexp: "{{ item.regexp }}"
        line: "{{ item.line }}"
        backrefs: yes
      with_items:
        - regexp: '^(disabled_plugins\s*=\s*\[s*)(\"cri\")(\s*\])$'
          line: '\1\3'
        - regexp: '^(\s*config_path)\s*=\s*.*$'
          line: '\1 = "/etc/containerd/certs.d"'
        - regexp: '^(\s*SystemdCgroup)\s*=\s*.*$'
          line: \1 = true
    - name: Add an apt signing key for opensuse repository
      become: true
      apt_key:
        url: https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable/Debian_11/Release.key
        state: present
    - name: Add devel:kubic:libcontainers:stable
      become: true
      ansible.builtin.apt_repository:
        repo: "deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/Debian_11/ /"
        state: present
        filename: "devel:kubic:libcontainers:stable"
    - name: Install crictl
      become: true
      apt:
        name: cri-tools
        state: present
        update_cache: true
    - name: Configure crictl to use containerd
      become: true
      copy:
        mode: '0644'
        dest: "/etc/crictl.yaml"
        content: |
          runtime-endpoint: unix:///var/run/containerd/containerd.sock
          image-endpoint: unix:///var/run/containerd/containerd.sock
          timeout: 10
          debug: false
    - name: Restart containerd
      become: true
      service:
        name: containerd
        state: restarted
- name: Set containerd snapshotter to work with ZFS
  hosts: zfs
  tasks:
  - name: Creates directory
    file:
      path: /var/lib/containerd/io.containerd.snapshotter.v1.zfs/
      state: directory
  - name: Create a zvol for snapshots
    become: "true"
    community.general.zfs:
      name: "main/containerd-snapshots"
      state: present
      extra_zfs_properties:
        mountpoint: /var/lib/containerd/io.containerd.snapshotter.v1.zfs
  - name: Set containerd configuration
    become: true
    ansible.builtin.lineinfile:
      path: /etc/containerd/config.toml
      regexp: '^(\s*snapshotter)\s*=\s*.*$'
      line: '\1 = "zfs"'
      backrefs: yes
    - name: Restart containerd
      become: true
      ansible.builtin.service:
        enabled: true
        name: containerd
        daemon_reload: true
        state: restarted
- name: Install kubernetes
  gather_facts: false
  become: true
  hosts: all
  vars:
    kubernetes_version: "1.24.3-00"
  tasks:
    # curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
    - name: Add an apt signing key for Kubernetes
      apt_key:
        url: https://packages.cloud.google.com/apt/doc/apt-key.gpg
        state: present
    # sudo add-apt-repository 'deb https://apt.kubernetes.io/ kubernetes-xenial main'
    - name: Adding apt repository for Kubernetes
      apt_repository:
        repo: deb https://apt.kubernetes.io/ kubernetes-xenial main
        state: present
        filename: kubernetes
    # sudo apt install kubelet kubeadm kubectl=2-00 kubectl=1.24.0-00
    # Truenas: sudo apt --fix-broken -o Dpkg::Options::="--force-overwrite" install
    - name: Install Kubernetes binaries
      apt:
        name: "{{ packages }}"
        state: present
        update_cache: true
      vars:
        packages:
          - "kubeadm={{ kubernetes_version }}"
          - "kubelet={{ kubernetes_version }}"
          - "kubectl={{ kubernetes_version }}"
    # sudo apt-mark hold kubeadm kubelet kubectl
    - name: APT Hold kebernetes packages
      dpkg_selections:
        name: "{{ item }}"
        selection: hold
      with_items:
        - kubeadm
        - kubelet
        - kubectl
    # TODO: Set up audit logs
    # # Preparation for audit logs
    # - name: Create a directory if it does not exist
    #   file:
    #     mode: '0644'
    #     owner: 'root'
    #     path: "{{ item }}"
    #     state: directory
    #   with_items:
    #     - /var/log/kubernetes/audit
    #     - /etc/kubernetes
    # - name: Set up audit policy
    #   copy:
    #     mode: '0644'
    #     owner: 'root'
    #     dest: "/etc/kubernetes/audit-policy.yaml"
    #     content: |
    #       apiVersion: audit.k8s.io/v1beta1
    #       kind: Policy
    #       rules:
    #       - level: Metadata
# RESET
# sudo kubeadm init phase certs all
# sudo ip route flush proto bird
# sudo rm -rf /var/etcd/calico-data/*
# sudo cp /etc/kubernetes/pki/ca.crt /usr/local/share/ca-certificates/kubernetes.crt && sudo update-ca-certificates
- name: Set up control plane
  gather_facts: true
  hosts: main
  vars:
    taint_control_plane: false
  tasks:
    - set_fact:
        ip: "{{ item }}"
      with_items: "{{ ansible_all_ipv4_addresses }}"
      when: "item.startswith('192') or item.startswith('172')"
    - name: Prepare init configuration
      become: true
      blockinfile:
        create: yes
        backup: yes
        dest: /var/lib/kubelet/config.init.yaml
        block: |
          apiVersion: kubeadm.k8s.io/v1beta2
          kind: ClusterConfiguration
          # --control-plane-endpoint
          controlPlaneEndpoint: {{ ip }}:6443
          ---
          apiVersion: kubelet.config.k8s.io/v1beta1
          kind: KubeletConfiguration
          cgroupDriver: systemd
          memorySwap:
            swapBehavior: LimitedSwap
          featureGates:
            NodeSwap: true
          failSwapOn: false
          networking:
            # --pod-network-cidr
            podSubnet: "10.244.0.0/16" 
          localAPIEndpoint:
            # --apiserver-advertise-address
            advertiseAddress: {{ ip }}
            bindPort: 6443
          ...
    - name: Restart kubelet
      become: true
      service:
        name: kubelet
        daemon_reload: true
        state: restarted
    # # Fixme: Run this only when necessary
    # #  kubeadm config images list -o jsonpath='{range .images[*]}{@}{"\n"}{end}'
    # #  Store available images as a fact?
    #- name: Pull config images
    #  become: true
    #  command: kubeadm config images pull
    # Fixme: Run this only when necessary
    - name: Initialize the Kubernetes cluster using kubeadm
      become: true
      # --apiserver-advertise-address="192.168.50.10"
      #  The IP address the API Server will advertise it's listening on.
      #  If not set the default network interface will be used
      # --contorl-plane-endpoint
      #  Specify a stable IP address or DNS name for the control plane.
      # --apiserver-cert-extra-sans="192.168.50.10"
      #  Optional extra Subject Alternative Names (SANs) to use for the API
      #  Server serving certificate. Can be both IP addresses and DNS names.
      # --node-name {{ node }}
      #  Specify the node name.
      # --pod-network-cidr=192.168.0.0/16
      #  Specify range of IP addresses for the pod network. If set, the
      #  control plane will automatically allocate CIDRs for every node.
      # --service-dns-domain="{{ cluster_domain }}" \
      shell:
        #--apiserver-cert-extra-sans="{{ ip }},127.0.0.1,localhost,{{ansible_hostname}}" \
        # --service-cidr=10.196.0.0/16 \
        # calico pod network cidr: 192.168.0.0/16.
        # --apiserver-cert-extra-sans="127.0.0.1" \
        # --apiserver-cert-extra-sans="127.0.0.1" \
        # --apiserver-cert-extra-sans="127.0.0.1" \
        cmd: |
          kubeadm init \
              --config=/var/lib/kubelet/config.init.yaml \
              --node-name={{ ansible_hostname }} \
              --skip-token-print
    - name: Create .kube directory for user
      become: true
      file:
        path: "{{ ansible_env.HOME }}/.kube"
        state: directory
        mode: '0700'
        owner: "{{ ansible_env.LOGNAME }}"
    - name: Setup kubeconfig for user
      become: true
      copy:
        src: /etc/kubernetes/admin.conf
        dest: "{{ ansible_env.HOME }}/.kube/config"
        owner: "{{ ansible_env.LOGNAME }}"
        remote_src: true
        mode: '0700'
    - name: Generate join command
      command: kubeadm token create --print-join-command
      register: join_command
      changed_when: false
      # Change to fetch
    - name: Copy join command to local file
      delegate_to: localhost
      become: false
      copy:
        content: "{{ join_command.stdout_lines[0] }}"
        mode: '0777'
        dest: "./join-command"
    - name: Untaint node
      command: "kubectl taint node {{ ansible_hostname }} {{ item }}-"
      when: not taint_control_plane
      with_items:
        - node-role.kubernetes.io/control-plane
        - node-role.kubernetes.io/master
- name: Set up worker node
  hosts: worker
  tasks:
  - name: Copy the join command to server location
    become: true
    copy:
      src: join-command
      dest: /tmp/join-command.sh
      mode: 0777
  # Run only as needed
  - name: Join the node to cluster
    become: true
    command: sh /tmp/join-command.sh
- name: "Install CNI: flannel"
  gather_facts: false
  become: false
  hosts: main
  tasks:
    - name: Create temporary directory
      ansible.builtin.tempfile:
        state: directory
        suffix: temporary
      register: temporary
    # sudo mkdir -p /opt/bin && sudo wget https://github.com/flannel-io/flannel/releases/download/v0.19.0/flanneld-amd64 --quiet --output-file=/opt/bin/flanneld && sudo chmod +x /opt/bin/flanneld
    - name: Download manifest to the cluster.
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
        dest: "{{ temporary.path }}/flannel.yaml"
        mode: '0664'
    # Fixme: Run this only when necessary
    - name: Install pod network
      kubernetes.core.k8s:
        state: present
        src: "{{ temporary.path }}/flannel.yaml"
    - name: Use the registered var and the file module to remove the temporary file
      ansible.builtin.file:
        path: "{{ temporary.path }}"
        state: absent
      when: temporary.path is defined
- name: "Install CNI: calico"
  gather_facts: false
  become: false
  hosts: main
  tasks:
    - name: Create temporary directory
      ansible.builtin.tempfile:
        state: directory
        suffix: temporary
      register: temporary
    - name: Download calico manifest to the cluster.
      ansible.builtin.get_url:
        url: https://docs.projectcalico.org/manifests/calico.yaml
        dest: "{{ temporary.path }}/calico.yaml"
        mode: '0664'
    # Fixme: Run this only when necessary
    - name: Install calico pod network
      kubernetes.core.k8s:
        state: present
        src: "{{ temporary.path }}/calico.yaml"
    - name: Use the registered var and the file module to remove the temporary file
      ansible.builtin.file:
        path: "{{ temporary.path }}"
        state: absent
      when: temporary.path is defined
- name: Install Helm
  gather_facts: false
  hosts: all
  tasks:
    - name: Add helm repository signing key
      become: true
      apt_key:
        url: https://baltocdn.com/helm/signing.asc
        # state: present
    - name: Add helm apt repository
      become: true
      apt_repository:
        repo: deb https://baltocdn.com/helm/stable/debian/ all main
        filename: helm
    - name: Install Helm
      become: true
      apt:
        name: "helm"
    - name: Install Helm diff plugin
      kubernetes.core.helm_plugin:
        plugin_path: https://github.com/databus23/helm-diff
        state: present
- name: Install jq
  gather_facts: false
  hosts: all
  tasks:
    - name: Install jq
      become: true
      apt:
        name: "jq"
- name: "Deploy CSI: NFS"
  gather_facts: false
  hosts: nfs
  vars_prompt:
    - name: nfsServer.apiKey
      private: true
    - name: nfsServer.ssh.key
      private: true
    - name: nfsServer.host
      default: truenas
  vars:
    nfsServer:
      ssh:
        username: root
      namespace: csi
      username: root
  tasks:
    # TODO: Do ZFS configuration
    # - name: Create a zvol for snapshots
    #   become: "true"
    #   community.general.zfs:
    #     #-snapshots
    #     name: "{{item}}"
    #     state: present
    #   with_items:
    #     - main/k8s
    #     - main/k8s/snapshots
    #     - main/k8s/volumes
    - name: Install CIFS utils
      become: true
      apt:
        name: "nfs-common"
        state: present
        update_cache: true
    - kubernetes.core.helm_repository:
        name: democratic-csi
        repo_url: https://democratic-csi.github.io/charts/
    - kubernetes.core.helm:
        create_namespace: true
        update_repo_cache: true
        name:  truenas-zfs-csi
        chart_ref: democratic-csi/democratic-csi
        release_namespace: "{{ nfsServer.namespace }}"
        values: "{{ lookup('template', './values/democratic-csi-freenas-api-nfs.yaml') | from_yaml }}"
- name: Install Rook/Ceph CSI
  gather_facts: false
  hosts: block
  vars:
    rook:
      namespace: rook-ceph-csi
  tasks:
    - kubernetes.core.helm_repository:
        name: rook-release
        repo_url: https://charts.rook.io/release
    - name: Deploy latest version of zfs-local-dataset chart inside democratic-csi namespace with values
      kubernetes.core.helm:
        create_namespace: true
        name: rook-ceph
        chart_ref: rook-release/rook-ceph
        release_namespace: "{{ rook.namespace }}"
      # TODO: Add kustomize with namespace https://raw.githubusercontent.com/rook/rook/master/deploy/examples/cluster.yaml
    - name: Deploy Rook/Ceph cluster
      command: "kubectl apply -f https://raw.githubusercontent.com/rook/rook/{{ rook_version }}/cluster/examples/kubernetes/ceph/cluster.yaml"
      # TODO: Add kustomize with namespace https://raw.githubusercontent.com/rook/rook/master/deploy/examples/dashboard-ingress-https.yaml
    - name: Deploy Ceph dashboard
      command: "kubectl apply -f https://raw.githubusercontent.com/rook/rook/{{ rook_version }}/cluster/examples/kubernetes/ceph/dashboard-loadbalancer.yaml"
      # https://rook-ceph-dashboard.local:8443/#/login
    - name: Create block manifest
      copy:
        dest: "/tmp/rook-ceph-block.yml"
        content: |
          apiVersion: ceph.rook.io/v1
          kind: CephBlockPool
          metadata:
            name: replicapool
            namespace: {{ namespace }}
          spec:
            failureDomain: host
            replicated:
              size: 3
          ---
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: rook-ceph-block
          # Change "rook-ceph" provisioner prefix to match the operator namespace if needed
          provisioner: rook-ceph.rbd.csi.ceph.com
          parameters:
              # clusterID is the namespace where the rook cluster is running
              clusterID: rook-ceph
              # Ceph pool into which the RBD image shall be created
              pool: replicapool

              # RBD image format. Defaults to "2".
              imageFormat: "2"

              # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only "layering" feature.
              imageFeatures: layering

              # The secrets contain Ceph admin credentials.
              csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
              csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
              csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
              csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
              csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

              # Specify the filesystem type of the volume. If not specified, csi-provisioner
              # will set default as "ext4". Note that "xfs" is not recommended due to potential deadlock
              # in hyperconverged settings where the volume is mounted on the same node as the osds.
              csi.storage.k8s.io/fstype: ext4

          # Delete the rbd volume when a PVC is deleted
          reclaimPolicy: Delete
    - name: Annotate Block storageclass as default
      command: "{{ item }}"
      with_items:
        - kubectl apply -f /tmp/rook-ceph-block.yml
        - "kubectl annotate StorageClass rook-ceph-block --namespace={{ namespace }} --overwrite storageclass.kubernetes.io/is-default-class=\"true\""
    - name: Create shared manifest
      copy:
        dest: "/tmp/rook-ceph-shared-filesystem.yml"
        content: |
          apiVersion: ceph.rook.io/v1
          kind: CephFilesystem
          metadata:
            name: myfs
            namespace: {{ namespace }}
            annotations:
              storageclass.kubernetes.io/is-default-class: "false"
          spec:
            metadataPool:
              replicated:
                size: 3
            dataPools:
              - replicated:
                  size: 3
            preservePoolsOnDelete: true
            metadataServer:
              activeCount: 1
              activeStandby: true
          ---
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: rook-cephfs
          # Change "rook-ceph" provisioner prefix to match the operator namespace if needed
          provisioner: rook-ceph.cephfs.csi.ceph.com
          parameters:
            # clusterID is the namespace where operator is deployed.
            clusterID: rook-ceph

            # CephFS filesystem name into which the volume shall be created
            fsName: myfs

            # Ceph pool into which the volume shall be created
            # Required for provisionVolume: "true"
            pool: myfs-data0

            # Root path of an existing CephFS volume
            # Required for provisionVolume: "false"
            # rootPath: /absolute/path

            # The secrets contain Ceph admin credentials. These are generated automatically by the operator
            # in the same namespace as the cluster.
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

          reclaimPolicy: Delete
    - name: Add Shared storageclass
      command: "{{ item }}"
      with_items:
        - kubectl apply -f /tmp/rook-ceph-shared-filesystem.yml
        - "kubectl annotate StorageClass rook-cephfs --namespace={{ namespace }} --overwrite storageclass.kubernetes.io/is-default-class=\"false\""
    - name: Create pod with tools
      command: "kubectl apply -f https://raw.githubusercontent.com/rook/rook/{{ rook_version }}/cluster/examples/kubernetes/ceph/toolbox.yaml"
      when: debug|bool
    - name: Get dashboard password
      shell:
        cmd: >
          kubectl -n {{ namespace }} get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
      register: dashboard_password
      when: debug|bool
    - name: Tools results
      shell:
        cmd: >
          kubectl -n {{ namespace }} exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') -- ceph status
      register: ceph_status
      when: debug == true
    - name: Show debug info
      when: debug|bool
      debug:
        msg: "{{ item }}"
      with_items:
        - "{{ dashboard_password.stdout }}"
        - "{{ ceph_status.stdout }}"
- name: Set up local ZFS CSI
  hosts: zfs
  vars:
    storageClass: zfs-local-dataset
  tasks:
    - kubernetes.core.helm_repository:
        name: democratic-csi
        repo_url: https://democratic-csi.github.io/charts/
    - name: Create a zvol for dataset
      become: true
      community.general.zfs:
        name: "{{ item }}"
        state: present
      with_items:
        - main/k8s/zfs-local/volumes
        - main/k8s/zfs-local/snapshots
    - name: Deploy latest version of zfs-local-dataset chart inside democratic-csi namespace with values
      kubernetes.core.helm:
        create_namespace: true
        name: "{{ storageClass }}"
        chart_ref: democratic-csi/democratic-csi
        release_namespace: democratic-csi
        values: "{{ lookup('template', './values/csi-democratic-csi-zfs-local.yaml') | from_yaml }}"
- name: Install metallb
  gather_facts: false
  hosts: main
  vars:
    metallb:
      namespace: loadbalancer
    address_range:
      from: 172.22.3.1
      to: 172.22.4.254
  tasks:
    - command: kubectl get configmap kube-proxy --namespace kube-system -o yaml
      register: configmap
    # If you’re using kube-proxy in IPVS mode, since Kubernetes v1.14.2 you have to enable strict ARP mode.
    - name: Enable strict ARP
      k8s:
        state: present
        definition: "{{configmap.stdout | replace ('strictARP: false','strictARP: true') | replace ('mode: \"\"','mode: \"ipvs\"') }}"
    - kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Namespace
          metadata:
            name: "{{metallb.namespace}}"
            labels:
              pod-security.kubernetes.io/enforce: privileged
              pod-security.kubernetes.io/audit: privileged
              pod-security.kubernetes.io/warn: privileged
    - kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: metallb.io/v1beta1
          kind: L2Advertisement
          metadata:
            name: advertisement
            namespace: "{{metallb.namespace}}"
    - kubernetes.core.k8s:
       state: present
       definition:
         apiVersion: metallb.io/v1beta1
         kind: IPAddressPool
         metadata:
           name: first-pool
           namespace: "{{metallb.namespace}}"
         spec:
           addresses:
           - 192.168.3.1-192.168.4.255
    - kubernetes.core.helm_repository:
        name: bitnami
        repo_url: https://charts.bitnami.com/bitnami
    - kubernetes.core.helm:
        name: metallb
        chart_ref: bitnami/metallb
        release_namespace: "{{metallb.namespace}}"
        update_repo_cache: true
- name: Install ExternalDNS (mDNS)
  gather_facts: false
  hosts: main-mdns
  tasks:
    - name: Install Avahi
      become: true
      apt:
        name: "avahi-daemon"
        update_cache: yes
    - name: 'Install External DNS: mDND manifest'
      command: kubectl apply -f https://raw.githubusercontent.com/tsaarni/k8s-external-mdns/master/external-dns-with-avahi-mdns.yaml
- name: Install ExternalDNS (CoreDNS)
  gather_facts: false
  hosts: main-coredns
  vars:
    externaldns:
      namespace: external-dns
    tld_hostname: home.arpa
  tasks:
    - name: "Create namespace"
      k8s:
        state: present
        kind: Namespace
        name: "{{ externaldns.namespace }}"
        api_version: v1
    - kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Secret
          metadata:
            name: etcd-client-tls
            namespace: external-dns
          data:
            client.pem: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUR5akNDQXJLZ0F3SUJBZ0lVRlA1K01lQzVsZzBkakEzZUVKVDRLdGJYQWR3d0RRWUpLb1pJaHZjTkFRRUwKQlFBd1N6RU1NQW9HQTFVRUJoTURTVTVVTVE0d0RBWURWUVFJRXdWRWRXMXRlVEVPTUF3R0ExVUVCeE1GUkhWdApiWGt4R3pBWkJnTlZCQU1URWt4dlkyRnNJR05zZFhOMFpYSWdSVlJEUkRBZUZ3MHlNakEzTVRNeE9EVXlNREJhCkZ3MHpNakEzTVRBeE9EVXlNREJhTUE4eERUQUxCZ05WQkFNVEJISnZiM1F3Z2dFaU1BMEdDU3FHU0liM0RRRUIKQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURsWEZrYTZ1U3l1TURvMGdUN2JXYUdLTU83N2lZMk13U2lTT3hoNHo0NgphOFJIa3lqRUJISHRSTm9VZTFpUVhkR3NidGZSTVhQVW92NUhPbGw3VHNuUEtLR0Jyd0M4Y2MvQndhSDIrSWVmCnJiaFdxMitPSkxxdDJsTEVydFM3TElTenovdkQ2SkdCa3IzY1BBZVNucVBpQzhCdzcvL05oZmYzcVFzWjU1ZnUKdVZuZUZKZVdsMlF2c1p0bEJjVi9oWVliTUE4aEllbnhVODVxd1UyVGllbHBjK2tBNWd0QkNOcEdGMHVpUmFROQpLNXBIRk5QMTgwNDEwMldTbG5lQzdkbi9tTE5pM3FXaEwxUVlWVHNiR3lsTlR5d2RhRGxlL2Q2czFtak5Ed0x1CjgxWUFya1JtTlcxS3c5NmJkM3VHQU5scElpK1BlRk94K3lmaDZtNU9tVzlsQWdNQkFBR2pnZUV3Z2Q0d0RnWUQKVlIwUEFRSC9CQVFEQWdXZ01CMEdBMVVkSlFRV01CUUdDQ3NHQVFVRkJ3TUJCZ2dyQmdFRkJRY0RBakFNQmdOVgpIUk1CQWY4RUFqQUFNQjBHQTFVZERnUVdCQlRGV0ozaWEvNllncklvZ3pFTFNGbktsUDV2blRBZkJnTlZIU01FCkdEQVdnQlNLelB4V0xOeHZGbzU0SC9FV3BKS1ZHajdSV2pCZkJnTlZIUkVFV0RCV2dnUmxkR05rZ2cxbGRHTmsKTFdobFlXUnNaWE56Z2dsc2IyTmhiR2h2YzNTQ0xpb3VaWFJqWkMxb1pXRmtiR1Z6Y3k1bGVIUmxjbTVoYkMxawpibk11YzNaakxtTnNkWE4wWlhJdWJHOWpZV3lIQkg4QUFBRXdEUVlKS29aSWh2Y05BUUVMQlFBRGdnRUJBQ0VCCldBRllLeTBaSTUrcm9SdHlqV2hrdy9LNlNEVVI2K2VYaklKRnBjem1KUVQvWXF4bVd1WmlRYXBtam9Lc2wwOEYKZXBBNEJwOWNxbk1Sa3pUN1Fwb2JQeUZYYjRZNU91L3NJUUwvUDFvUUdFODhWZEJuZEl0YjBOa3FLRGNmbkQ2SAo3NzIyMGt3eFZuL3RLTjRWeFdiMHVjU3pPQUM1YzZPdFJSZUl1Rkx1NWZ2OFJ3dmtFT0toU1BDdThiTlJKRFVTCkNuVkdYaTRaaU9TblhCVk1QbFlPN0FmaGwrOUJLc2ZpRzdQUXVPVXRpbVVGR01PNnI5RWFldkR4MnBSRWFCRVoKNUtpSStNenJPMXgyOHg0QUJPTmJ6amwwUmRyQmRGZEdtTnE2NzdsZTI4Y0E4Tmd6UTBaMmZpaWdJVUNxSWxkbQpYc3FhOFVIZWY2TDFCcEVJcTY0PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
            ca.pem: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURaakNDQWs2Z0F3SUJBZ0lVQTIrMDhyZEp2cWU2eFJvZGN3VzRIdWJtVFNVd0RRWUpLb1pJaHZjTkFRRUwKQlFBd1N6RU1NQW9HQTFVRUJoTURTVTVVTVE0d0RBWURWUVFJRXdWRWRXMXRlVEVPTUF3R0ExVUVCeE1GUkhWdApiWGt4R3pBWkJnTlZCQU1URWt4dlkyRnNJR05zZFhOMFpYSWdSVlJEUkRBZUZ3MHlNakEzTVRNeE9EVXlNREJhCkZ3MHlOekEzTVRJeE9EVXlNREJhTUVzeEREQUtCZ05WQkFZVEEwbE9WREVPTUF3R0ExVUVDQk1GUkhWdGJYa3gKRGpBTUJnTlZCQWNUQlVSMWJXMTVNUnN3R1FZRFZRUURFeEpNYjJOaGJDQmpiSFZ6ZEdWeUlFVlVRMFF3Z2dFaQpNQTBHQ1NxR1NJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURLa0wrVEExdWppQlZ1bUtWQWRBR09KQWJ0ClRaNk9aR3k5ZEVMUWJsWGF4M2MzaG16SGVZL2tYKytub29pWHJIamFndWt5RWVUb0xMV1E0VjRFa0I4M3ZUaTMKWDV3NkJzckluTld3d21RaCtkM3NTeW1LOW9PZkt4NXRBazNWcGNqYng2d2YyR1hNTm1uUE04WWU0SWlxV0JOcwptQkRNREFURDhVek5xU2hGUEkyUE9SUlRqOUp4bkZUVGtsWm9mQmlXNGt2aUZGT1NrQzhjK25mMWkzZjU2WG1iCmNEdzBsTTVTeWJ0blo5RnlpK01pOEt1RUo3ZjlnSHdqd2wvWG53c1hFWUcrRFJvekJRSG5NdWhqWFBKckNPWFMKWDVYM0xCd0VkcUQ5UG8zRnBrUkZSallvUENzR0U2d2pjTVV6M25WT29yQklkN09zcXF3Z0ZtM1p4Tm16QWdNQgpBQUdqUWpCQU1BNEdBMVVkRHdFQi93UUVBd0lCQmpBUEJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXCkJCU0t6UHhXTE54dkZvNTRIL0VXcEpLVkdqN1JXakFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBaGZ3RHlHNEIKMkZRQVA3cUlPWE10YmxJNjdOeHF0Y0ZNZWZjcnllNWE1bjRkelV0L1d3TDNiYXc3NU1CN0swU0NZQTBRdG5PQgpvWUNKNlpIWEpNclFnZTM1TlhrTVpvUTRCOHJnSi81Y1RwRlkyNmpMdWl5T1RYTUFIVlk1Y0dLQmZHRzNGbXprCklNNVBaZW5nMFRMSmlONVE2WGJqRk42L1dwNW96QzFuU2pNYTBQbkZiUE9lQTFjNG0xT29nZ21FZ2VHdGdlQk8KenUzbmgzM2t2aUMvS0Q2UmprbS8wbEZBYmF5WGpSam93djBoRW9HV3ZLK2hGZFdxRWYxM2QxRnV3Yzg0MlZKeAo0bVoycTNoamhCdVNONFJ2OHlNZzU3MXlLTXQ5NUVwUmdYSUVjNngzeEMwQ29jbjNOaEtBNTBNU1ZBZXZDL2ZiClJzdzArZjFkL0RobkZBPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
            client-key.pem: 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
    - kubernetes.core.helm_repository:
        name: "{{ item.name }}"
        repo_url: "{{ item.repo }}"
      with_items:
        - name: bitnami
          repo: https://charts.bitnami.com/bitnami
        - name: coredns
          repo: https://coredns.github.io/helm
    - kubernetes.core.helm:
        name: dummy
        namespace: kube-system
        state: absent
        update_repo_cache: true
    - kubernetes.core.helm:
        name: "{{ item.name }}"
        create_namespace: true
        chart_ref: "{{ item.chart }}"
        release_namespace: "{{ externaldns.namespace }}"
        values: "{{ lookup('template', './values/external-dns-' + item.value + '.yml') | from_yaml }}"
      with_items:
        - value: coredns
          chart: coredns/coredns
          name: coredns
        - value: external-dns
          chart: bitnami/external-dns
          name: external-dns
        - value: etcd
          chart: bitnami/etcd
          name: etcd
    - kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: v1
          kind: Service
          metadata:
            namespace: "{{ externaldns.namespace }}"
            name: external-dns-public
            annotations:
              kubernetes.io/ingress.class: nginx
          spec:
            type: LoadBalancer
            ports:
              - port: 53
                targetPort: 53
                protocol: UDP
                name: udp
            selector:
              app.kubernetes.io/name: coredns
    # Rewrite as Ansible
    - ansible.builtin.shell: |
        #!/bin/bash
        mkdir /tmp/coredns
        kubectl get \
            --namespace=kube-system \
            --output=jsonpath='{.data.Corefile}' \
            configmap/coredns  \
            > /tmp/coredns/Corefile
        echo "{{ tld_hostname }}:53 {
            errors
            cache 30
            forward . $(kubectl get service \
              --namespace="{{ externaldns.namespace }}" \
              --output=jsonpath='{.status.loadBalancer.ingress[0].ip}' \
              external-dns-public)
        }" >> /tmp/coredns/Corefile
        kubectl patch \
            --namespace=kube-system \
            configmap/coredns \
            --patch="{\"data\":{\"Corefile\":\"$(cat /tmp/coredns/Corefile | awk -v ORS='\\n' '1')\"}}"
        rm -rf /tmp/coredns
- name: Install ingress
  gather_facts: false
  hosts: main
  vars:
    tld_hostname: home.arpa
    ingress_namespace: ingress
    ingress_release: ingress
    ingress_repository:
      name: ingress-nginx
      url: https://kubernetes.github.io/ingress-nginx
  tasks:
    - kubernetes.core.helm_repository:
        name: "{{ingress_repository.name}}"
        repo_url: "{{ingress_repository.url}}"
    - name: Install ingress
      kubernetes.core.helm:
        update_repo_cache: true
        name: "{{ingress_release}}"
        create_namespace: true
        chart_ref: "{{ingress_repository.name}}/ingress-nginx"
        release_namespace: "{{ ingress_namespace }}"
    - name: Instructions
      debug:
        msg: |
          To use, add the annotation to ingress
            kubernetes.io/ingress.class: nginx
            external-dns.alpha.kubernetes.io/hostname: "{{ tld_hostname }}"
- name: Install dashboard
  gather_facts: false
  hosts: main
  vars:
    dashboard:
      namespace: dashboard
      name: dashboard
    tld_hostname: home.arpa
  tasks:
    - kubernetes.core.helm_repository:
        name: kubernetes-dashboard
        repo_url: https://kubernetes.github.io/dashboard/
    - kubernetes.core.helm:
        update_repo_cache: true
        name: "{{ dashboard.name }}"
        create_namespace: true
        chart_ref: kubernetes-dashboard/kubernetes-dashboard
        release_namespace: "{{ dashboard.namespace }}"
        values: "{{ lookup('template', './values/dashboard-values.yaml') | from_yaml }}"
    # Ingress is not namespaced in the helm chart
    - kubernetes.core.k8s:
        definition:
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            annotations:
              cert-manager.io/cluster-issuer: ca-issuer
              kubernetes.io/ingress.class: nginx
              nginx.ingress.kubernetes.io/backend-protocol: HTTPS
              service.alpha.kubernetes.io/app-protocols: '{"https":"HTTPS"}'
            name: "dashboard-kubernetes-dashboard"
            namespace: "{{ dashboard.namespace }}"
          spec:
            rules:
              - host: "{{ dashboard.name }}.{{ tld_hostname }}"
                http:
                  paths:
                    - backend:
                        service:
                          name: "dashboard-kubernetes-dashboard"
                          port:
                            number: 443
                      path: /
                      pathType: ImplementationSpecific
            tls:
              - hosts:
                  - "{{ dashboard.name }}.{{ tld_hostname }}"
                secretName: "dashboard-tls"
        state: present
    - kubernetes.core.k8s:
        definition: "{{ lookup('template', './values/dashboard-user-{{ item }}.yaml') | from_yaml }}"
        state: present
      with_items:
        - crb
        - sa
    - name: Password to dashboard
      shell:
        cmd: "kubectl -n {{ dashboard.namespace }} create token admin-user"
      register: dashboard_password
    - debug:
        msg: "{{dashboard_password.stdout}}"
# TODO: Make ldap declarative
- name: Deploy LDAP
  gather_facts: true
  hosts: main
  vars:
    LDAP_BASE_DN: "{{'{{ LDAP_BASE_DN }}'}}"
    LDAP_DOMAIN: "{{'{{ LDAP_DOMAIN }}'}}"
    LDAP_READONLY_USER_USERNAME: "{{'{{ LDAP_READONLY_USER_USERNAME }}'}}"
    LDAP_READONLY_USER_PASSWORD_ENCRYPTED: "{{'{{ LDAP_READONLY_USER_PASSWORD_ENCRYPTED }}'}}"
    tld_hostname: home.arpa
    ldap:
      ldif: "{{ lookup('template', './values/ldap.ldif') }}"
      namespace: ldap
      organization: "My Company"
      openldap:
        releaseName: openldap
      phpldapadmin:
        releaseName: phpldapadmin
  vars_prompt:
    - name: admin_password
      default: "secret"
      private: yes
      prompt: Ldap admin password
  tasks:
    - kubernetes.core.helm_repository:
        name: "{{ item.name }}"
        repo_url: "{{ item.url }}"
      with_items:
        - name: stable
          url: https://charts.helm.sh/stable
        - name: cetic
          url: https://cetic.github.io/helm-charts
    - kubernetes.core.helm:
        name: dummy
        namespace: kube-system
        state: absent
        update_repo_cache: true
    - kubernetes.core.helm:
        name: "{{ item.name }}"
        create_namespace: true
        chart_ref: "{{ item.chartRef }}"
        release_namespace: "{{ ldap.namespace }}"
        values: "{{ lookup('template', './values/ldap-{{ item.name }}.yml') | from_yaml }}"
      with_items:
        - name: "{{ ldap.phpldapadmin.releaseName }}"
          chartRef: cetic/phpldapadmin
        - name: "{{ ldap.openldap.releaseName }}"
          chartRef: stable/openldap
    - kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: phpldapadmin-ingress
            namespace: "{{ ldap.namespace }}"
            annotations:
              kubernetes.io/ingress.class: "nginx"
              cert-manager.io/cluster-issuer: ca-issuer
          spec:
            rules:
              - host: "ldap-admin.{{ tld_hostname }}"
                http:
                  paths:
                    - backend:
                        service:
                          name: "{{ ldap.phpldapadmin.releaseName }}"
                          port:
                            number: 80
                      pathType: ImplementationSpecific
            tls:
              - hosts:
                  - "ldap-admin.{{ tld_hostname }}"
                secretName: ldap-admin-tls
- name: Deploy monitoring
  gather_facts: false
  hosts: main
  vars:
    monitoring:
      namespace: monitoring
    opensearch:
      opensearch-dashboards:
        name: opensearch
    tld_hostname: home.arpa
  tasks:
    - kubernetes.core.helm_repository:
        name: "{{ item.name }}"
        repo_url: "{{ item.url }}"
      with_items:
        - name: grafana
          url: https://grafana.github.io/helm-charts
        - name: loki
          url: https://grafana.github.io/loki/charts
        - name: prometheus-community
          url: https://prometheus-community.github.io/helm-charts
        - name: opensearch
          url: https://opensearch-project.github.io/helm-charts
    - kubernetes.core.helm:
        name: dummy
        namespace: kube-system
        state: absent
        update_repo_cache: true
    - kubernetes.core.helm:
        name: "{{ item.release_name }}"
        create_namespace: true
        chart_ref: "{{ item.repository }}"
        release_namespace: "{{ monitoring.namespace }}"
        values: "{{ lookup('template', './values/monitoring-{{ item.release_name }}.yaml') | from_yaml }}"
      with_items:
        - release_name: grafana
          repository: grafana/grafana
        - release_name: loki
          repository: loki/loki
        - release_name: prometheus
          repository: prometheus-community/prometheus
        - release_name: promtail
          repository: loki/promtail
        - release_name: "opensearch"
          repository: "opensearch/opensearch"
        - release_name: opensearch-dashboards
          repository: "opensearch/opensearch-dashboards"
    # TODO: Handle ingress for prometheus via Helm chart
    - kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: networking.k8s.io/v1
          kind: Ingress
          metadata:
            name: grafana
            namespace: "{{ monitoring.namespace }}"
            annotations:
              kubernetes.io/ingress.class: "nginx"
              cert-manager.io/cluster-issuer: ca-issuer
          spec:
            tls:
              - hosts:
                  - "grafana.{{ tld_hostname }}"
                secretName: "grafana-tls"
            rules:
              - host: "grafana.{{ tld_hostname }}"
                http:
                  paths:
                    - backend:
                        service:
                          name: grafana
                          port:
                            number: 80
                      pathType: ImplementationSpecific
- name: Deploy cert manager
  gather_facts: true
  hosts: main
  vars:
    INTERMEDIATE_CA_FILENAME: company-intermediate-ca
    certmanager:
      namespace: "cert-manager"
      name: cert-manager
  tasks:
    - kubernetes.core.helm_repository:
        name: jetstack
        repo_url: https://charts.jetstack.io
    - kubernetes.core.helm:
        create_namespace: true
        update_repo_cache: true
        name: "{{ certmanager.name }}"
        chart_ref: jetstack/cert-manager
        release_namespace: "{{ certmanager.namespace }}"
        values:
          installCRDs: true
    - kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: cert-manager.io/v1
          kind: ClusterIssuer
          metadata:
            name: ca-issuer
            namespace: "{{ certmanager.namespace }}"
          spec:
            ca:
              secretName: ca-key-pair
    - name: Create temporary build directory
      ansible.builtin.tempfile:
        state: directory
        suffix: certs
      register: tempdir
    # Rewrite as Ansible
    - become: true
      ansible.builtin.shell:
        chdir: "{{ tempdir.path }}"
        cmd: |
          #!/bin/bash
          
          set -e
          
          CA_PATH="/etc/kubernetes/pki"
          
          # Generate key for intermediate CA
          #  -aes256 to protect it with passphrase
          openssl genrsa -out "{{INTERMEDIATE_CA_FILENAME}}.key" 4096
          
          cat <<EOF > openssl.cnf
          [req]
          req_extensions = v3_req
          # req_extensions = v3_intermediate_ca
          # x509_extensions = v3_req
          x509_extensions = v3_intermediate_ca
          distinguished_name = req_distinguished_name
          prompt = no
          
          [ v3_req ]
          extendedKeyUsage = serverAuth, clientAuth, codeSigning, emailProtection
          basicConstraints = critical,CA:TRUE
          keyUsage = nonRepudiation, digitalSignature, keyEncipherment
          
          [req_distinguished_name]
          countryName            = PL
          stateOrProvinceName    = Mazovia
          localityName           = Pruszkow
          organizationName       = Home
          commonName             = home.arpa intermediate CA
          emailAddress           = jacek.lipiec.bc@gmail.com
          
          [ v3_intermediate_ca ]
          subjectKeyIdentifier = hash
          authorityKeyIdentifier = keyid:always,issuer
          basicConstraints = critical,CA:true,pathlen:0
          keyUsage = critical,digitalSignature,cRLSign, keyCertSign
          EOF
          
          # Generate itermediate CSR for home.arpa
          openssl req \
              -config openssl.cnf \
              -new \
              -sha256 \
              -newkey rsa:2048 \
              -nodes \
              -key "{{INTERMEDIATE_CA_FILENAME}}.key" \
              -out "{{INTERMEDIATE_CA_FILENAME}}.csr"
          
          # Sign request with kubernetes
          openssl x509 \
              -req \
              -in "{{INTERMEDIATE_CA_FILENAME}}.csr" \
              -CA "${CA_PATH}/ca.crt" \
              -CAkey "${CA_PATH}/ca.key" \
              -extensions v3_intermediate_ca \
              -CAcreateserial \
              -extfile openssl.cnf \
              -out "{{INTERMEDIATE_CA_FILENAME}}.crt" \
              -days 500 \
              -sha512
          
          cat "{{INTERMEDIATE_CA_FILENAME}}.crt" "${CA_PATH}/ca.crt" > chain.crt
          chown {{ansible_env.LOGNAME}} chain.crt "{{INTERMEDIATE_CA_FILENAME}}.key"
    - ansible.builtin.shell:
        chdir: "{{ tempdir.path }}"
        cmd: |
          kubectl create secret generic \
              --namespace "{{ certmanager.namespace }}" \
              ca-key-pair \
              --from-file=tls.crt=./chain.crt \
              --from-file=tls.key={{INTERMEDIATE_CA_FILENAME}}.key
    - name: Use the registered var and the file module to remove the temporary file
      ansible.builtin.file:
        path: "{{ tempdir.path }}"
        state: absent
      when: tempdir.path is defined
- name: Deploy Gerrit
  gather_facts: false
  hosts: main
  vars:
    HTTPD_LISTEN_URL: "proxy-https://*:8080/"
    TEMP_DIRECTORY: "/tmp/gerrit"
    LDAP_SERVER: "ldap://openldap.ldap"
    LDAP_DC: "dc=home,dc=arpa"
    gerrit:
      release_name: gerrit
      namespace: gerrit
    tld_hostname: home.arpa
    storageClass: freenas-nfs-csi
  tasks:
    - ansible.builtin.tempfile:
        state: directory
        suffix: certs
      register: tempdir
    - ansible.builtin.git:
        repo: https://github.com/GerritCodeReview/k8s-gerrit.git
        dest: "{{tempdir.path}}"
    - kubernetes.core.helm:
        create_namespace: true
        name: "{{gerrit.release_name}}"
        chart_ref: "{{tempdir.path}}/helm-charts/gerrit"
        release_namespace: "{{ gerrit.namespace }}"
        values: "{{ lookup('template', './values/gerrit-values.yml') | from_yaml }}"
    - name: Annotate service with external hostname
      command: |
        kubectl annotate Service {{gerrit.release_name}}-gerrit-service \
          --namespace={{ gerrit.namespace }} \
          --overwrite external-dns.alpha.kubernetes.io/hostname="ssh.{{gerrit.release_name}}.{{tld_hostname}}"
    - ansible.builtin.file:
        path: "{{ tempdir.path }}"
        state: absent
      when: tempdir.path is defined
...