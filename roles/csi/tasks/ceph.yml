- kubernetes.core.helm_repository:
    name: rook-release
    repo_url: https://charts.rook.io/release
- name: Deploy latest version of zfs-local-dataset chart inside democratic-csi namespace with values
  kubernetes.core.helm:
    create_namespace: true
    name: rook-ceph
    chart_ref: rook-release/rook-ceph
    release_namespace: "{{ rook.namespace }}"
  # TODO: Add kustomize with namespace https://raw.githubusercontent.com/rook/rook/master/deploy/examples/cluster.yaml
- name: Deploy Rook/Ceph cluster
  command: "kubectl apply -f https://raw.githubusercontent.com/rook/rook/{{ rook_version }}/cluster/examples/kubernetes/ceph/cluster.yaml"
  # TODO: Add kustomize with namespace https://raw.githubusercontent.com/rook/rook/master/deploy/examples/dashboard-ingress-https.yaml
- name: Deploy Ceph dashboard
  command: "kubectl apply -f https://raw.githubusercontent.com/rook/rook/{{ rook_version }}/cluster/examples/kubernetes/ceph/dashboard-loadbalancer.yaml"
  # https://rook-ceph-dashboard.local:8443/#/login
- name: Create block manifest
  copy:
    dest: "/tmp/rook-ceph-block.yml"
    content: |
      apiVersion: ceph.rook.io/v1
      kind: CephBlockPool
      metadata:
        name: replicapool
        namespace: {{ namespace }}
      spec:
        failureDomain: host
        replicated:
          size: 3
      ---
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: rook-ceph-block
      # Change "rook-ceph" provisioner prefix to match the operator namespace if needed
      provisioner: rook-ceph.rbd.csi.ceph.com
      parameters:
          # clusterID is the namespace where the rook cluster is running
          clusterID: rook-ceph
          # Ceph pool into which the RBD image shall be created
          pool: replicapool
      
          # RBD image format. Defaults to "2".
          imageFormat: "2"
      
          # RBD image features. Available for imageFormat: "2". CSI RBD currently supports only "layering" feature.
          imageFeatures: layering
      
          # The secrets contain Ceph admin credentials.
          csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
          csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
          csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
          csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
          csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
      
          # Specify the filesystem type of the volume. If not specified, csi-provisioner
          # will set default as "ext4". Note that "xfs" is not recommended due to potential deadlock
          # in hyperconverged settings where the volume is mounted on the same node as the osds.
          csi.storage.k8s.io/fstype: ext4
      
      # Delete the rbd volume when a PVC is deleted
      reclaimPolicy: Delete
- name: Annotate Block storageclass as default
  command: "{{ item }}"
  with_items:
    - kubectl apply -f /tmp/rook-ceph-block.yml
    - "kubectl annotate StorageClass rook-ceph-block --namespace={{ namespace }} --overwrite storageclass.kubernetes.io/is-default-class=\"true\""
- name: Create shared manifest
  copy:
    dest: "/tmp/rook-ceph-shared-filesystem.yml"
    content: |
      apiVersion: ceph.rook.io/v1
      kind: CephFilesystem
      metadata:
        name: myfs
        namespace: {{ namespace }}
        annotations:
          storageclass.kubernetes.io/is-default-class: "false"
      spec:
        metadataPool:
          replicated:
            size: 3
        dataPools:
          - replicated:
              size: 3
        preservePoolsOnDelete: true
        metadataServer:
          activeCount: 1
          activeStandby: true
      ---
      apiVersion: storage.k8s.io/v1
      kind: StorageClass
      metadata:
        name: rook-cephfs
      # Change "rook-ceph" provisioner prefix to match the operator namespace if needed
      provisioner: rook-ceph.cephfs.csi.ceph.com
      parameters:
        # clusterID is the namespace where operator is deployed.
        clusterID: rook-ceph
      
        # CephFS filesystem name into which the volume shall be created
        fsName: myfs
      
        # Ceph pool into which the volume shall be created
        # Required for provisionVolume: "true"
        pool: myfs-data0
      
        # Root path of an existing CephFS volume
        # Required for provisionVolume: "false"
        # rootPath: /absolute/path
      
        # The secrets contain Ceph admin credentials. These are generated automatically by the operator
        # in the same namespace as the cluster.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
      
      reclaimPolicy: Delete
- name: Add Shared storageclass
  command: "{{ item }}"
  with_items:
    - kubectl apply -f /tmp/rook-ceph-shared-filesystem.yml
    - "kubectl annotate StorageClass rook-cephfs --namespace={{ namespace }} --overwrite storageclass.kubernetes.io/is-default-class=\"false\""
- name: Create pod with tools
  command: "kubectl apply -f https://raw.githubusercontent.com/rook/rook/{{ rook_version }}/cluster/examples/kubernetes/ceph/toolbox.yaml"
  when: debug|bool
- name: Get dashboard password
  shell:
    cmd: >
      kubectl -n {{ namespace }} get secret rook-ceph-dashboard-password -o jsonpath="{['data']['password']}" | base64 --decode && echo
  register: dashboard_password
  when: debug|bool
- name: Tools results
  shell:
    cmd: >
      kubectl -n {{ namespace }} exec -it $(kubectl -n rook-ceph get pod -l "app=rook-ceph-tools" -o jsonpath='{.items[0].metadata.name}') -- ceph status
  register: ceph_status
  when: debug == true
- name: Show debug info
  when: debug|bool
  debug:
    msg: "{{ item }}"
  with_items:
    - "{{ dashboard_password.stdout }}"
    - "{{ ceph_status.stdout }}"
