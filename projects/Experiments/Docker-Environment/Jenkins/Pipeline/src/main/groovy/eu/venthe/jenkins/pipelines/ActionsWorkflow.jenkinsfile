package eu.venthe.jenkins.pipelines

import com.cloudbees.groovy.cps.NonCPS
import com.fasterxml.jackson.databind.ObjectMapper
import com.fasterxml.jackson.dataformat.yaml.YAMLFactory

// TODO: Rewrite passwords as vault
properties([parameters([
        string(name: 'EXECUTION_ENVIRONMENT', defaultValue: 'docker', description: ''),
        stashedFile(name: 'INPUT', default: null, description: ''),
        text(name: 'INPUT_TEST', default: null, description: ''),
        string(name: 'GERRIT_URL', defaultValue: 'ssh://admin@host.docker.internal:29418'),
        string(name: 'NEXUS_URL', defaultValue: 'http://host.docker.internal:8081'),
        string(name: 'NEXUS_USERNAME', defaultValue: 'admin'),
        string(name: 'NEXUS_PASSWORD', defaultValue: 'secret'),
        string(name: 'GIT_SSH_COMMAND2', defaultValue: 'ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -i "~/.ssh/id_rsa"')
])])

@Grapes([
        @Grab('com.fasterxml.jackson.core:jackson-databind:2.13.3'),
        @Grab('com.fasterxml.jackson.dataformat:jackson-dataformat-yaml:2.13.3'),
        @Grab('com.fasterxml.jackson.datatype:jackson-datatype-jsr310:2.13.3')
])
@NonCPS

// FIXME: Remove custom implementation of YAML parser
//  snakeYaml from jenkins converts 'on' to true...
static Map parseYaml(input) {
    def mapper = new ObjectMapper(new YAMLFactory())
    def parsedJson = mapper.readTree(input)
    return mapper.convertValue(parsedJson, Map.class)
}

def runOn(environment, body) {
    if (params.EXECUTION_ENVIRONMENT != environment) {
        return
    }
    body.call()
}

def globalExecutor(body) {
    node('built-in') {
        runOn("kubernetes") {
            podTemplate {
                node(POD_TEMPLATE) {
                    ansiColor('xterm') {
                        body.call()
                    }
                }
            }
        }
        runOn("docker") {
            docker.image("docker.io/venthe/ubuntu-runner:22.10").inside("--entrypoint=''") {
                ansiColor('xterm') {
                    body.call()
                }
            }
        }
    }
}

def runner(image, body) {
    runOn("kubernetes") {
        podTemplate(yaml: """\
                        kind: Pod
                        spec:
                          containers:
                            - name: runner
                              image: ${image}
                              imagePullPolicy: Always
                              securityContext:
                                privileged: true
                              command:
                                - sleep
                              args:
                                - 9999999""".stripIndent()) {
            node(POD_TEMPLATE) {
                container("runner") {
                    ansiColor('xterm') {
                        body.call()
                    }
                }
            }
        }
    }
    runOn("docker") {
        def volumes = '-v "C:\\Users\\jacek\\Desktop\\_Pipeline\\Action-Runner\\dist\\index.mjs:/runner/index.mjs"'
        docker.image(image).inside("""--entrypoint='' --interactive --tty ${volumes}""") {
            ansiColor('xterm') {
                body.call()
            }
        }
    }
}

def runOnNode() {
    sh 'stdbuf -i0 -o0 -e0 node \${RUNNER_MANAGER_DIRECTORY}/index.mjs'
}

static def handles(events, input) {
    if (events == null) {
        return false
    }

    println "handles " + events.inspect()

    return events
            .collect { it.toLowerCase() }
            .contains(input["type"].toLowerCase())
}

def withEnvironmentVariables(workflowFile, jobName) {
    // FIXME: Rewrite as withEnv
    //  With env is not working on docker node?
    writeFile file: "ci.env", text: [
            'GERRIT_SSH'       : "${params.GERRIT_URL}",
            'GIT_SSH_COMMAND'  : "${params.GIT_SSH_COMMAND2}",
            'GIT_SSH_VARIANT'  : "ssh",
            "PIPELINE_WORKFLOW": "${workflowFile}",
            "PIPELINE_JOB_NAME": "${jobName}",
            "NEXUS_URL": "${params.NEXUS_URL}",
            "NEXUS_USERNAME": "${params.NEXUS_USERNAME}",
            "NEXUS_PASSWORD": "${params.NEXUS_PASSWORD}"
    ].collect { "${it.key}=${it.value}" }
            .join('\n')
}

def prepareWorkflow(workflowFile, input) {
    try {
        println "Loading workflow: ${workflowFile.toString()}"
        def file = readFile workflowFile.toString()
        def workflow = parseYaml(file)
        if (workflow == null) {
            throw new Exception("Workflow should not be null")
        }

        def workflowName = workflow['name'] ? "${workflow['name']} - ${workflowFile.name}" : workflowFile.name
        def on = workflow['on']

        if (on == null) {
            println "We are not handling this due to lack of ON"
            throw new Exception("There is no \"on\" property, this workflow will never run")
        }
        if (Map.isAssignableFrom(on.getClass())) {
            def events = on['event']
            if (events != null) {
                println "We are handling a map" + on.inspect()

                if (Collection.isAssignableFrom(events.getClass())) {
                    if (!handles(events, input)) {
                        print "Event ${input['type']} not handled in ${workflow['on']}"
                        return
                    }
                } else {
                    if (!handles([events], input)) {
                        print "Event ${input['type']} not handled in ${workflow['on']}"
                        return
                    }
                }
            } else {
                throw new Exception("Should not happen, as we always should handle those events" + on.inspect())
            }

        } else if (Collection.isAssignableFrom(on.getClass())) {
            println "We are handling a list" + workflow['on'].inspect()
            if (!handles(workflow['on'], input)) {
                print "Event ${input['type']} not handled in ${workflow['on']}"
                return
            }
        } else {
            throw new Exception("2 On unrecognized. Should not happen ${on.getClass()} ${on.getClass() instanceof LinkedHashMap}")
        }

        def action = {
            workflow['jobs'].each { jobName, job ->
                runner(job['runs-on']) {
                    cleanWs()
                    unstash 'INPUT'
                    withEnvironmentVariables(workflowFile.toString(), jobName)
                    withCredentials(bindings: [sshUserPrivateKey(credentialsId: 'gerrit',               \
                                                           keyFileVariable: 'SSH_KEY_FOR_ABC',               \
                                                           passphraseVariable: '',               \
                                                           usernameVariable: '')]) {
                        sh "mkdir -p /root/.ssh && cp ${SSH_KEY_FOR_ABC} /root/.ssh/id_rsa"
                    }
                    sh "chmod u=rwx,g=,o= ~/.ssh/*"
                    sh "cp INPUT /runner/metadata/INPUT"
                    stage("[${workflow['name']}/${jobName}]") {
                        runOnNode()
                    }
                }
            }
        }
        return ["${workflowName}": action]
    } catch (Exception e) {
        return ["${workflowFile.toString()}": {
            stage("${workflowFile.toString()}") {
                error("File content:\n${readFile(workflowFile.toString())}\n\n${e.getMessage()}\n${printStackTrace(e)}")
            }
        }]
    }
}

// FIXME: Rewrite as standard Jenkins log
//  to have a stack pretty printed, we use our own stack trace prettifier
def printStackTrace(exception) {
    StringWriter writer = new StringWriter();
    PrintWriter printWriter = new PrintWriter(writer);
    exception.printStackTrace(printWriter);
    printWriter.flush();

    return writer.toString();
}

def checkoutProperties(project1, branch1, ref1) {
    return [$class           : 'GitSCM',
            branches         : [
                    [name: "${ref1}"]
            ],
            extensions       : [
                    [$class: 'LocalBranch', localBranch: "**"],
                    [$class: 'BuildSingleRevisionOnly'],
                    [$class: 'CloneOption', shallow: true, honorRefspec: true],
                    [$class: 'SparseCheckoutPaths', sparseCheckoutPaths: [
                            [$class: 'SparseCheckoutPath', path: '.pipeline/']
                    ]]
            ],
            userRemoteConfigs: [
                    [credentialsId: 'gerrit',
                     refspec      : "+${branch1}:refs/remotes/origin/${branch1} +${ref1}:refs/remotes/origin/${ref1}",
                     url          : "${params.GERRIT_URL}/${"${project1}"}"
                    ]
            ]
    ]
}

globalExecutor {
    def input
    stage("Load input") {
        cleanWs()
        if (!((INPUT_TEST?.trim()).isEmpty())) {
            println "Stashing debug input: \"${INPUT_TEST}\" ${INPUT_TEST?.getClass()}"
            writeJSON file: 'INPUT', json: INPUT_TEST
            stash 'INPUT'
        }
        unstash 'INPUT'
        input = readJSON file: "INPUT"
        // TODO: Null check INPUT
    }
    def workflows
    stage("Prepare workflows") {
        println "Checking out actions"
        def branch1 = "${input['change']['branch']}"
        def ref1 = "${input['patchSet']['ref']}"
        def project1 = "${input['change']['project']}"
        def type1 = "${input['type']}"
        currentBuild.displayName = "#${BUILD_NUMBER} [${project1}] ${type1}: +${branch1}:${ref1}"
        currentBuild.description = "The best description."

        checkout(checkoutProperties(project1, branch1, ref1))
        workflows = findFiles(glob: '.pipeline/workflows/*.y*ml')
                .collect { prepareWorkflow(it, input) }
                .findAll { it != null }
                .collectEntries { it }
        workflows.failFast = false
    }
    stage("Run jobs") {
        parallel workflows
    }
}
